{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# csv 파일 읽기\n",
    "df = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Amount_max_fraud 컬럼 생성\n",
    "df['Amount_max_fraud'] = 1\n",
    "df.loc[df.Amount <= 2125.87, 'Amount_max_fraud'] = 0\n",
    "\n",
    "#Drop all of the features that have very similar distributions between the two types of transactions.\n",
    "df = df.drop(['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8'], axis =1)\n",
    "\n",
    "#Based on the plots above, these features are created to identify values where fraudulent transaction are more common.\n",
    "# fraudulent transaction 이 더 잘나타는 방향으로 자료 정리 V1_~ V21_ 생성\n",
    "df['V1_'] = df.V1.map(lambda x: 1 if x < -3 else 0)\n",
    "df['V2_'] = df.V2.map(lambda x: 1 if x > 2.5 else 0)\n",
    "df['V3_'] = df.V3.map(lambda x: 1 if x < -4 else 0)\n",
    "df['V4_'] = df.V4.map(lambda x: 1 if x > 2.5 else 0)\n",
    "df['V5_'] = df.V5.map(lambda x: 1 if x < -4.5 else 0)\n",
    "df['V6_'] = df.V6.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V7_'] = df.V7.map(lambda x: 1 if x < -3 else 0)\n",
    "df['V9_'] = df.V9.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V10_'] = df.V10.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V11_'] = df.V11.map(lambda x: 1 if x > 2 else 0)\n",
    "df['V12_'] = df.V12.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V14_'] = df.V14.map(lambda x: 1 if x < -2.5 else 0)\n",
    "df['V16_'] = df.V16.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V17_'] = df.V17.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V18_'] = df.V18.map(lambda x: 1 if x < -2 else 0)\n",
    "df['V19_'] = df.V19.map(lambda x: 1 if x > 1.5 else 0)\n",
    "df['V21_'] = df.V21.map(lambda x: 1 if x > 0.6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new feature for normal (non-fraudulent) transactions.\n",
    "# Normal 컬럼 생성 Class가 0 이면 1, Class가 1이면 0\n",
    "df.loc[df.Class == 0, 'Normal'] = 1\n",
    "df.loc[df.Class == 1, 'Normal'] = 0\n",
    "\n",
    "# Rename 'Class' to 'Fraud'\n",
    "# Class -> Fraud 로 명 변환\n",
    "df = df.rename(columns = {'Class' : 'Fraud'})\n",
    "\n",
    "# 492 fraudulent transactions, 284,315 normal transactions.\n",
    "# 0.172% of transactions were fraud.\n",
    "print(df.Normal.value_counts())\n",
    "print()\n",
    "print(df.Fraud.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max column 수 설정\n",
    "pd.set_option('display.max_columns',  101)\n",
    "# print(df.head())\n",
    "\n",
    "# Create dataframes of only Fraud and Normal transactions.\n",
    "Fraud = df[df.Fraud == 1]\n",
    "Normal = df[df.Normal == 1]\n",
    "print('Fraud  : ', len(Fraud))\n",
    "print('Normal : ', len(Normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set X_train equal to 80% of the fraudulent transactions.\n",
    "FraudSample  = Fraud.sample(frac=0.8)\n",
    "NormalSample = Normal.sample(frac=0.8)\n",
    "count_Frauds = len(FraudSample)\n",
    "# Add 80% of the normal transactions to X_train.\n",
    "for_train = pd.concat([FraudSample, NormalSample], axis=0)\n",
    "\n",
    "# X_test contains all the transaction not in X_train.\n",
    "for_test = df.loc[~df.index.isin(for_train.index)]\n",
    "\n",
    "print('len(for_train)  : ',len(for_train))\n",
    "print('len(for_test)   : ',len(for_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle the dataframes so that the training is done in a random order.\n",
    "for_train = for_train.sample(frac=1).reset_index(drop=True)\n",
    "for_test = for_test.sample(frac=1).reset_index(drop=True)\n",
    "#for_test = np.random.shuffle(for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add our target features to y_train and y_test.\n",
    "X_train = for_train.drop(['Fraud', 'Normal'], axis = 1)\n",
    "# Drop target features from X_train and X_test.\n",
    "#  Fraud, Normal 컬럼 drop\n",
    "y_train = for_train[['Fraud', 'Normal']]\n",
    "\n",
    "# Add our target features to y_train and y_test.\n",
    "X_test = for_test.drop(['Fraud', 'Normal'], axis = 1)\n",
    "# Drop target features from X_train and X_test.\n",
    "#  Fraud, Normal 컬럼 drop\n",
    "y_test = for_test[['Fraud', 'Normal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check to ensure all of the training/testing dataframes are of the correct length\n",
    "print('len(X_train) : ',len(X_train))\n",
    "print('len(y_train) : ',len(y_train))\n",
    "print('len(X_test)  : ',len(X_test))\n",
    "print('len(y_test)  : ',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In [26]\n",
    "'''\n",
    "Due to the imbalance in the data, ratio will act as an equal weighting system for our model. \n",
    "By dividing the number of transactions by those that are fraudulent, ratio will equal the value that when multiplied\n",
    "by the number of fraudulent transactions will equal the number of normal transaction. \n",
    "Simply put: # of fraud * ratio = # of normal\n",
    "'''\n",
    "\n",
    "ratio = len(X_train) / count_Frauds\n",
    "print('ratio :', ratio)\n",
    "ratio = 10\n",
    "y_train.Fraud *= ratio\n",
    "y_test.Fraud *= ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Names of all of the features in X_train.\n",
    "features = X_train.columns.values\n",
    "print('features : ',features)\n",
    "\n",
    "#Transform each feature in features so that it has a mean of 0 and standard deviation of 1;\n",
    "#this helps with training the neural network.\n",
    "for feature in features:\n",
    "    mean, std = df[feature].mean(), df[feature].std()\n",
    "    # print('feature :',feature , 'mean : ', mean , 'std :', std)\n",
    "    X_train.loc[:, feature] = (X_train[feature] - mean) / std\n",
    "    X_test.loc[:, feature] = (X_test[feature] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "'''\n",
    "Train the Neural Net\n",
    "'''\n",
    "\n",
    "# In [28]\n",
    "# Split the testing data into validation and testing sets\n",
    "split = int(len(y_test)/2)\n",
    "print('split : ', split)\n",
    "\n",
    "inputX = X_train.as_matrix()\n",
    "inputY = y_train.as_matrix()\n",
    "inputX_valid = X_test.as_matrix()[:split]\n",
    "inputY_valid = y_test.as_matrix()[:split]\n",
    "inputX_test = X_test.as_matrix()[split:]\n",
    "inputY_test = y_test.as_matrix()[split:]\n",
    "\n",
    "print('y_train.Normal.value_counts() :',y_train.Normal.value_counts())\n",
    "print('y_train.Fraud.value_counts() :',y_train.Fraud.value_counts())\n",
    "\n",
    "print('y_test.Normal.value_counts() :',y_test.Normal.value_counts())\n",
    "print('y_test.Fraud.value_counts() :',y_test.Fraud.value_counts())\n",
    "\n",
    "print('C inputY_valid', np.where(inputY_valid[:, 0] > 0, 1, 0).sum())\n",
    "print('C inputY_test ', np.where(inputY_test[:, 0] > 0, 1, 0).sum())\n",
    "\n",
    "print('inputX :',inputX.shape)\n",
    "print('inputY :',inputY.shape)\n",
    "print('inputX_valid :',inputX_valid.shape)\n",
    "print('inputY_valid :',inputY_valid.shape)\n",
    "print('inputX_test :',inputX_test.shape)\n",
    "print('inputY_test :',inputY_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of input nodes.\n",
    "input_nodes = 37\n",
    "\n",
    "# Multiplier maintains a fixed ratio of nodes between each layer.\n",
    "mulitplier = 1.5\n",
    "\n",
    "# Number of nodes in each hidden layer\n",
    "hidden_nodes1 = 18\n",
    "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
    "hidden_nodes3 = round(hidden_nodes2 * mulitplier)\n",
    "\n",
    "# Percent of nodes to keep during dropout.\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# In [30]\n",
    "x = tf.placeholder(tf.float32, [None, input_nodes])\n",
    "\n",
    "# layer 1\n",
    "#W1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_nodes1], stddev= 0.15))\n",
    "W1 = tf.get_variable(name='W1', initializer=tf.contrib.layers.xavier_initializer(), shape=[input_nodes, hidden_nodes1])\n",
    "b1 = tf.Variable(tf.zeros(hidden_nodes1))\n",
    "y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "# layer 2\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev= 0.15))\n",
    "b2 = tf.Variable(tf.zeros(hidden_nodes2))\n",
    "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "# layer 3\n",
    "W3 = tf.Variable(tf.truncated_normal([hidden_nodes2, hidden_nodes3], stddev= 0.15))\n",
    "b3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "y3 = tf.nn.sigmoid(tf.matmul(y2, W3) + b3)\n",
    "y3 = tf.nn.dropout(y3, pkeep)\n",
    "\n",
    "# layer 4\n",
    "W4 = tf.Variable(tf.truncated_normal([hidden_nodes3, 2], stddev= 0.15))\n",
    "b4 = tf.Variable(tf.zeros([2]))\n",
    "y4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)\n",
    "\n",
    "# output\n",
    "y = y4\n",
    "y_ = tf.placeholder(tf.float32, [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In [31]\n",
    "# Parameters\n",
    "training_epochs = 5             # should be 2000, it will timeout when uploading\n",
    "training_dropout = 0.9          # drop out\n",
    "display_step = 1                # 10\n",
    "n_samples = y_train.shape[0]\n",
    "batch_size = 2048\n",
    "learning_rate = 0.005           # 하이퍼파라미터\n",
    "\n",
    "# Cost function: Cross Entropy 손실 함수\n",
    "cost = -tf.reduce_sum(y_ * tf.log(y))\n",
    "\n",
    "# We will optimize our model via AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Correct prediction if the most likely value (Fraud or Normal) from softmax equals the target value. 정확도\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "predict_by_nn = tf.nn.softmax(y)\n",
    "decision_by_nn = tf.argmax(y, 1)\n",
    "actual = tf.argmax(y_, 1)\n",
    "# Note: some code will be commented out below that relate to saving/checkpointing your model.\n",
    "\n",
    "# In [34]\n",
    "accuracy_summary = []           # Record accuracy values for plot\n",
    "cost_summary = []               # Record cost values for plot\n",
    "valid_accuracy_summary = []\n",
    "valid_cost_summary = []\n",
    "stop_early = 0                  # To keep track of the number of epochs before early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the best weights so that they can be used to make the final predictions\n",
    "#checkpoint = \"location_on_your_computer/best_model.ckpt\"\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "# Initialize variables and tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        for batch in range(int(n_samples/batch_size)):\n",
    "            # print('batch :', batch)\n",
    "            batch_x = inputX[batch * batch_size : (1 + batch) * batch_size]\n",
    "            batch_y = inputY[batch * batch_size : (1 + batch) * batch_size]\n",
    "\n",
    "            sess.run([optimizer], feed_dict={x: batch_x, y_: batch_y, pkeep: training_dropout})\n",
    "\n",
    "        # Display logs after every 10 epochs\n",
    "        if(epoch) % display_step == 0:\n",
    "            train_accuracy, newCost = sess.run([accuracy, cost], feed_dict={x: inputX, y_: inputY, pkeep: training_dropout})\n",
    "            valid_accuracy, valid_newCost = sess.run([accuracy, cost], feed_dict={x: inputX_valid, y_:inputY_valid, pkeep: 1})\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  'Acc  = ',\"{:.5f}\".format(train_accuracy),\n",
    "                  'Cost = ','{:.5f}'.format(newCost),\n",
    "                  'Valid_Acc =', '{:.5f}'.format(valid_accuracy),\n",
    "                  'Valid_Cost =', '{:.5f}'.format(valid_newCost)\n",
    "                  )\n",
    "\n",
    "            # Record the results of the model\n",
    "            accuracy_summary.append(train_accuracy)\n",
    "            cost_summary.append(newCost)\n",
    "            valid_accuracy_summary.append(valid_accuracy)\n",
    "            valid_cost_summary.append(valid_newCost)\n",
    "\n",
    "            # If the model does not improve after 15 logs, stop the training.\n",
    "            if valid_accuracy < max(valid_accuracy_summary) and epoch > 100:\n",
    "                stop_early += 1\n",
    "                if stop_early == 15:\n",
    "                    break\n",
    "            else:\n",
    "                stop_early = 0\n",
    "\n",
    "    test_predict_by_nn, test_decision_by_nn, test_actual = sess.run([predict_by_nn, decision_by_nn, actual], feed_dict={x: inputX_test, y_: inputY_test, pkeep: 1})\n",
    "    print('test_predict_by_nn  :', test_predict_by_nn)\n",
    "    print('test_decision_by_nn :', test_decision_by_nn)\n",
    "    print('test_actual         :', test_actual)\n",
    "\n",
    "    res = [(a[0], a[1], b,c) for a,b,c in zip(test_predict_by_nn, test_decision_by_nn, test_actual)]\n",
    "    resdf = pd.DataFrame(data=res, columns=['Fr', 'Nm', 'NN', 'ACTUAL'])\n",
    "    print(resdf.columns)\n",
    "\n",
    "    TP = resdf[(resdf.NN == 0) & (resdf.ACTUAL == 0)].values.shape[0]\n",
    "    FP = resdf[(resdf.NN == 0) & (resdf.ACTUAL == 1)].values.shape[0]\n",
    "    TN = resdf[(resdf.NN == 1) & (resdf.ACTUAL == 1)].values.shape[0]\n",
    "    FN = resdf[(resdf.NN == 1) & (resdf.ACTUAL == 0)].values.shape[0]\n",
    "\n",
    "    print('TP', TP)\n",
    "    print('FP', FP)\n",
    "    print('TN', TN)\n",
    "    print('FN', FN)\n",
    "    print('Acc', (TP+TN)/(TP+TN+FP+FN))\n",
    "    print('Precision', TP/(TP+FP))\n",
    "    print('Recall', TP/(TP+FN))\n",
    "    resdf.to_csv('predict.csv', index=False)\n",
    "    # print(resdf[:100])\n",
    "\n",
    "print()\n",
    "print(\"Optimization Finished!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In [35]\n",
    "# Plot the accuracy and cost summaries\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,4))\n",
    "\n",
    "ax1.plot(accuracy_summary)          # blue\n",
    "ax1.plot(valid_accuracy_summary)    # green\n",
    "ax1.set_title('Accuracy')\n",
    "\n",
    "ax2.plot(cost_summary)\n",
    "ax2.plot(valid_cost_summary)\n",
    "ax2.set_title('Cost')\n",
    "\n",
    "plt.xlabel('Epochs (x10)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
