{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Links:\n",
    "  [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "  [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import os\n",
    "import DATA\n",
    "\n",
    "##\n",
    "'''\n",
    "predict_size 파라미터가 추가. 2017/10/31\n",
    "'''\n",
    "class LSTM:\n",
    "    @staticmethod\n",
    "    def RNN(x, weights, biases, windowsize, num_hidden):\n",
    "      x = tf.unstack(x, windowsize, 1)\n",
    "      lstm_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(num_hidden, forget_bias=1.0),\n",
    "                                    rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)])\n",
    "      outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "      \n",
    "      return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "    \n",
    "  \n",
    "    def __init__(self, input_dim, window_size, num_hidden, output_dim,\n",
    "                 predict_size=1,\n",
    "                 name='lstm',\n",
    "                 loss='square',\n",
    "                 opt='grad'):\n",
    "        '''\n",
    "        LSTM모델의 입력차수, 윈도우 크기, hidden layer의 개수, 출력차수에 따라 모델구성 변수들을 생성하고,\n",
    "        학습의 오차(loss) 계산식 정의, 최적화 함수 정의를 한다.\n",
    "\n",
    "        :param input_dim:\n",
    "        :param window_size:\n",
    "        :param num_hidden:\n",
    "        :param output_dim:\n",
    "        :param predict_size:\n",
    "        :param name:\n",
    "        :param loss:\n",
    "        :param opt:\n",
    "        '''\n",
    "        # Network Parameters\n",
    "        self.input_dim = input_dim #\n",
    "        self.window_size = window_size #\n",
    "        self.num_hidden = num_hidden #\n",
    "        self.output_dim = output_dim #\n",
    "        self.predict_size = predict_size #\n",
    "\n",
    "        self.valid_x = None\n",
    "        self.valid_y = None\n",
    "        self.valid_stop = 0 # 학습종료 판단하는 validation 기준값\n",
    "        \n",
    "        self.fig_num = 0 # for plotting\n",
    "\n",
    "        # clear all things in tensorflow\n",
    "        tf.reset_default_graph()\n",
    "        # tf Graph input\n",
    "        self.X = tf.placeholder(\"float\", [None, self.window_size, self.input_dim])\n",
    "        self.Y = tf.placeholder(\"float\", [None, self.predict_size * self.output_dim])\n",
    "        \n",
    "        # Define weights\n",
    "        high = 4*np.sqrt(6.0/(self.num_hidden + self.output_dim))\n",
    "        t = tf.Variable(tf.random_uniform([self.num_hidden, self.predict_size * self.output_dim], minval=-high, maxval=high, dtype=tf.float32))\n",
    "        self.weights = {\n",
    "            #'out': tf.Variable(tf.random_normal([self.num_hidden, self.output_dim]))\n",
    "            # 참고 - https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-10-3-mnist_nn_xavier.py\n",
    "            # http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "            # xavier initialization\n",
    "            #'out': tf.get_variable(\"W\", shape=[self.num_hidden, self.output_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            'out': t\n",
    "        }\n",
    "        self.biases = {\n",
    "            'out': tf.Variable(tf.random_normal([self.predict_size * self.output_dim]))\n",
    "        }\n",
    "\n",
    "        # self.prediction == logits\n",
    "        self.prediction = LSTM.RNN(self.X, self.weights, self.biases, self.window_size, self.num_hidden)\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        starter_learning_rate = 0.1\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                   200, 0.97, staircase=True)\n",
    "        \n",
    "        ## Define loss and optimizer\n",
    "        # self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        #  logits=self.logits, labels=self.Y))\n",
    "        print('LOSS:', loss, ' OPT:', opt)\n",
    "        if loss == 'abs':\n",
    "          self.loss_op = tf.reduce_mean(tf.abs(self.prediction - self.Y))\n",
    "        elif loss == 'softmax':\n",
    "          self.loss_op = tf.reduce_mean(tf.nn.softmax(self.prediction))\n",
    "        elif loss == 'softmax_entropy':\n",
    "          self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "              logits=self.prediction, labels=self.Y))\n",
    "        else:\n",
    "          self.loss_op = tf.reduce_mean(tf.square(self.prediction - self.Y))\n",
    "        \n",
    "        if opt == 'grad':\n",
    "          self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        elif opt == 'adam':\n",
    "          self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        elif opt == 'rms':\n",
    "          self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate)\n",
    "        else:\n",
    "          assert(False)\n",
    "          \n",
    "        self.train_op = self.optimizer.minimize(self.loss_op, global_step=self.global_step)\n",
    "        \n",
    "        # Evaluate model (with test logits, for dropout to be disabled)\n",
    "        self.accuracy = self.loss_op\n",
    "        self.name_network = self.build_model_name(name)\n",
    "        self.training_stop = None\n",
    "      \n",
    "    def set_name(self, name):\n",
    "        self.name_network = self.build_model_name(name)\n",
    "  \n",
    "    def build_model_name(self, name):\n",
    "        return '%s-W%d-H%d-I%d-O%d' % \\\n",
    "               (name, self.window_size, self.num_hidden, self.input_dim, \n",
    "\t\t\tself.output_dim * self.predict_size)\n",
    "    \n",
    "    def save(self):\n",
    "        fname = '%s%s.ckpt' % (CFG.NNMODEL, self.name_network)\n",
    "        self.saver = tf.train.Saver()\n",
    "        save_path = self.saver.save(self.sess, fname)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    def load(self):\n",
    "        fname = '%s%s.ckpt' % (CFG.NNMODEL, self.name_network)\n",
    "        if not os.path.isfile(fname+'.index'):\n",
    "          print('Model NOT found', fname)\n",
    "          return False\n",
    "        \n",
    "        # Run the initializer\n",
    "        self.sess = tf.Session()\n",
    "        #self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.saver.restore(self.sess, fname)\n",
    "        print(\"Model restored from file: %s\" % fname)\n",
    "        return True\n",
    "    \n",
    "    def set_training_stop(self, training_stop):\n",
    "      self.training_stop = training_stop\n",
    "      \n",
    "    def set_validation_data(self, valid_x, valid_y, valid_stop=0):\n",
    "        if (valid_x is None) or (valid_x.shape[0] == 0):\n",
    "          return\n",
    "        self.valid_x = valid_x\n",
    "        self.valid_y = valid_y\n",
    "        self.valid_stop = valid_stop # validation 결과값이 valid_stop 값 이하이면 학습 종료\n",
    "      \n",
    "    def do_validation(self):\n",
    "        if self.valid_x is None or self.valid_x.shape[0] == 0:\n",
    "          return 0\n",
    "        valid_acc = self.do_test(self.valid_x, self.valid_y, 'Validation')\n",
    "        return valid_acc\n",
    "        \n",
    "    \n",
    "    def run(self, training_x, training_y, epochs=1000, batch_size=0, display_step=100):\n",
    "        '''\n",
    "        LSTM모델의 학습(training)을 수행하는 함수이다. training 데이터, validation 데이터를 별도로 지정할 수 있다.\n",
    "        training 데이터 전체에서  batch_size만큼의 입력 및 출력 데이터를 1회의 batch training에 사용한다.\n",
    "        batch를 일정 회수(display_step)만큼 수행한 후 validation 수치를 계산한다.\n",
    "        학습 종료 조건으로 최대 epoch를 지정하거나,  validation목표 오차를 지정할 수 있다.\n",
    "\n",
    "        :param training_x:\n",
    "        :param training_y:\n",
    "        :param epochs:\n",
    "        :param batch_size:\n",
    "        :param display_step:\n",
    "        :return:\n",
    "        '''\n",
    "        self.max_epochs = epochs\n",
    "        if batch_size == 0:\n",
    "          batch_size = int(training_x.shape[0] * 0.05)\n",
    "        self.display_step = display_step\n",
    "\n",
    "        # Run the initializer\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        #training_y =  training_y.reshape((-1, self.predict_size * self.output_dim))\n",
    "        training = DATA.BatchDataGen(training_x, training_y)\n",
    "        \n",
    "        for step in range(1, self.max_epochs+1):\n",
    "            batch_x, batch_y = training.next_batch(batch_size)\n",
    "            \n",
    "            self.sess.run(self.train_op, feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "            if step % self.display_step == 0 or step == 1:\n",
    "                loss, acc = self.sess.run([self.loss_op, self.accuracy],\n",
    "                                          feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "                try:\n",
    "                  curr_lr = self.sess.run(self.optimizer._learning_rate)\n",
    "                except:\n",
    "                  curr_lr = self.sess.run(self.optimizer._lr)\n",
    "                \n",
    "                print(\"Step \" + str(step) + \": Acc= \" + \"{:.6f}\".format(acc) + \\\n",
    "\t\t\t                \", LR= \" + \"{:.6f}\".format(curr_lr))\n",
    "\n",
    "                if self.training_stop is not None and acc < self.training_stop:\n",
    "                    print('STOP by training_stop')\n",
    "                    break\n",
    "                  \n",
    "                valid_res = self.do_validation()\n",
    "                if self.valid_stop != 0 and valid_res < self.valid_stop :\n",
    "                    print('STOP by valid_stop')\n",
    "                    break\n",
    "        \n",
    "        return acc, valid_res # training_error, validation_error\n",
    "        \n",
    "      \n",
    "        \n",
    "    def do_test(self, test_x, test_y, mesg='Test'):\n",
    "        acc = self.sess.run(self.accuracy, feed_dict={self.X: test_x, self.Y: test_y})\n",
    "        print(\"%s: %.6f\" % (mesg, acc))\n",
    "        return acc\n",
    "    \n",
    "    def do_compare(self, test_x, test_y):\n",
    "        predict_y = self.sess.run(self.prediction, feed_dict={self.X: test_x, self.Y: test_y})\n",
    "        diff = np.abs(predict_y - test_y) / test_y\n",
    "        return diff\n",
    "\n",
    "    def predict(self, test_x):\n",
    "      predict_y = self.sess.run(self.prediction, feed_dict={self.X: test_x})\n",
    "      #for a, p in zip(test_y[:20], predict_y[:20]):\n",
    "      #  print('A, P', a, p)\n",
    "      return predict_y\n",
    "      \n",
    "    def close(self):\n",
    "      self.sess.close()\n",
    "      \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
